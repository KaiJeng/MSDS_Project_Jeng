---
title: "MSDS_Project_Jeng"
author: "Darren Jeng"
date: "November 28, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(stringr)
```

## Video Game Sales vs Reviews


```{r copies}
# When extracting the list, there are over 205 elements; only two of the elements contain the full list needed, the rest were just titles of the games. 
url <- "http://www.vgchartz.com/yearly/2016/USA/"
USA_YEARLY_GET <- url %>% read_html() %>% html_nodes("table") %>% html_table(fill = TRUE)
USA_Copies_Yearly_2016 <- USA_YEARLY_GET[[2]]
names(USA_Copies_Yearly_2016) <- make.names(names(USA_Copies_Yearly_2016))
names(USA_Copies_Yearly_2016)[1] <- "Position"
names(USA_Copies_Yearly_2016)[2] <- "Remove"
names(USA_Copies_Yearly_2016)[4] <- "Remove"
names(USA_Copies_Yearly_2016)[5] <- "Remove"
names(USA_Copies_Yearly_2016)[6] <- "Remove"
names(USA_Copies_Yearly_2016)[7] <- "Weeks"
names(USA_Copies_Yearly_2016)[8] <- "Yearly"
names(USA_Copies_Yearly_2016)[9] <- "Total"

USA_Copies_Yearly_2016_Cleaned <- USA_Copies_Yearly_2016[,c("Position", "Game", "Weeks", "Yearly", "Total")]




# Trying to get aggrgate score for 2016
url <- "http://www.gamerankings.com/browse.html?site=&cat=0&year=2016&numrev=1&sort=0&letter=&search="
SCORE_GET <- url %>% read_html() %>% html_nodes("table") %>% html_table(fill = TRUE)
SCORE_GET <- as.data.frame(SCORE_GET)
# Rbind for appending
for (i in 1:9){
  url <- paste("http://www.gamerankings.com/browse.html?page=", i, "&year=2016&numrev=1", sep = "")
  SUBSCORE_GET <- url %>% read_html %>% html_nodes("table") %>% html_table(fill = TRUE)
  SUBSCORE_GET <- as.data.frame(SUBSCORE_GET)
  SCORE_GET <- rbind(SCORE_GET,SUBSCORE_GET)
}
# Start to tidy data set
SCORE_GET$X1 <- NULL
colnames(SCORE_GET)[colnames(SCORE_GET) == "X2"] <- "Platform"
colnames(SCORE_GET)[colnames(SCORE_GET) == "X3"] <- "Game.Untidy"
colnames(SCORE_GET)[colnames(SCORE_GET) == "X4"] <- "Score.Untidy"
# Remove escape sequences
SCORE_GET$Game.Untidy <- str_replace_all(SCORE_GET$Game.Untidy,"\r\n\t\t","$")
# Tidy Game.Untidy to Game, Developer, and Publisher
SCORE_GET <- separate(SCORE_GET, Game.Untidy, into = c("Game", "Info"), sep = "\\$")
SCORE_GET$Info <- str_replace(SCORE_GET$Info,"\\, 2016","")
SCORE_GET$Info <- str_replace(SCORE_GET$Info,"Eighting\\/Raizing", "Eighting")
# Some developers self publish their game which is why a publisher is not listed for those games. Let's fix that.
SCORE_GET <- separate(SCORE_GET, Info, into = c("Developer", "Publisher"), sep = "/")
for (i in 1:nrow(SCORE_GET)) {
  if (is.na(SCORE_GET$Publisher[i] == TRUE)) {SCORE_GET$Publisher[i] <- SCORE_GET$Developer[i]}
}
# Tidy Score.Untiy to Score and Number.Of.Reviews
SCORE_GET$Score <- str_extract(SCORE_GET$Score.Untidy,"[0-9]+.[0-9]+")
SCORE_GET$Number.of.Reviews <- str_sub(SCORE_GET$Score.Untidy, 7, 8)
SCORE_GET$Score.Untidy <- NULL


# Now we need to get aggregate scores for 2015 
url <- "http://www.gamerankings.com/browse.html?site=&cat=0&year=2015&numrev=1&sort=0&letter=&search="
SCORE_GET2 <- url %>% read_html() %>% html_nodes("table") %>% html_table(fill = TRUE)
SCORE_GET2 <- as.data.frame(SCORE_GET2)
# Rbind for appending
for (i in 1:10){
  url <- paste("http://www.gamerankings.com/browse.html?page=", i, "&year=2015&numrev=1", sep = "")
  SUBSCORE_GET2 <- url %>% read_html %>% html_nodes("table") %>% html_table(fill = TRUE)
  SUBSCORE_GET2 <- as.data.frame(SUBSCORE_GET2)
  SCORE_GET2 <- rbind(SCORE_GET2,SUBSCORE_GET2)
}
# Start to tidy data set
SCORE_GET2$X1 <- NULL
colnames(SCORE_GET2)[colnames(SCORE_GET2) == "X2"] <- "Platform"
colnames(SCORE_GET2)[colnames(SCORE_GET2) == "X3"] <- "Game.Untidy"
colnames(SCORE_GET2)[colnames(SCORE_GET2) == "X4"] <- "Score.Untidy"
# Remove escape sequences and have a separator symbol to be used
SCORE_GET2$Game.Untidy <- str_replace_all(SCORE_GET2$Game.Untidy,"\r\n\t\t","$")
# Tidy Game.Untidy to Game, Developer, and Publisher
SCORE_GET2 <- separate(SCORE_GET2, Game.Untidy, into = c("Game", "Info"), sep = "\\$")
SCORE_GET2$Info <- str_replace(SCORE_GET2$Info,"\\, 2015","")
SCORE_GET2 <- separate(SCORE_GET2, Info, into = c("Developer", "Publisher"), sep = "/")
# Some developers self publish their game which is why a publisher is not listed for those games. Let's fix that. For any row that has N/A in Publisher, copy publisher to developer.
for (i in 1:nrow(SCORE_GET2)) {
  if (is.na(SCORE_GET2$Publisher[i] == TRUE)) {SCORE_GET2$Publisher[i] <- SCORE_GET2$Developer[i]}
}
# Tidy Score.Untiy to Score and Number.Of.Reviews
SCORE_GET2$Score <- str_extract(SCORE_GET2$Score.Untidy,"[0-9]+.[0-9]+")
SCORE_GET2$Number.of.Reviews <- str_sub(SCORE_GET2$Score.Untidy, 7, 8)
SCORE_GET2$Score.Untidy <- NULL


# Merge the 2015 and 2016 reviews into one list, ordered by score
SCORE_GET <- rbind(SCORE_GET, SCORE_GET2)
SCORE_GET$Score <- as.numeric(SCORE_GET$Score)
SCORE_GET <- SCORE_GET[order(-SCORE_GET$Score),]
rownames(SCORE_GET) <- NULL
  
  
# Getting sales data for software and hardware
url <- "http://www.vgchartz.com/weekly/42372/USA/"
SALES_GET <- url %>% read_html() %>% html_nodes("table") %>% html_table(fill = TRUE)
# There is already a graph for total sales of software and hardware. We can pull that first then worry about individual software next.
SALES_CONSOLE_GET <- SALES_GET[[154]]
SALES_TOTAL_SOFTWARE_GET <- SALES_GET[[155]]
SALES_WEEKLY_SOFTWARE_GET <- SALES_GET[[2]]

# With these three data sets initialized, we need a for loop that can extract these three sets for each week until the most current data avaliable. Since the "date" portion of the url just increases by seven, we can just have the loop keep going by intervals of seven until it reaches the most current data. 

# Why not just grab all the information into 

for (i in seq(from = 42379, to = 42687, by = 7)) {
  url <- paste("http://www.vgchartz.com/weekly/", i, "/USA/", sep = "")
}

# Tidy software sales
names(SALES_WEEKLY_SOFTWARE_GET) <- make.names(names(SALES_WEEKLY_SOFTWARE_GET))
colnames(SALES_WEEKLY_SOFTWARE_GET)[colnames(SALES_WEEKLY_SOFTWARE_GET) == "Weekly"] <- "Remove"
colnames(SALES_WEEKLY_SOFTWARE_GET)[colnames(SALES_WEEKLY_SOFTWARE_GET) == "Total"] <- "Remove2"
colnames(SALES_WEEKLY_SOFTWARE_GET)[colnames(SALES_WEEKLY_SOFTWARE_GET) == "Week.."] <- "Weekly"
names(SALES_WEEKLY_SOFTWARE_GET)[6] <- "Total"
names(SALES_WEEKLY_SOFTWARE_GET)[7] <- "Week.Number"

# Two games are from 2015 and are labeled as such with parenthesis. Change that to brackets for easier separating later on.
SALES_WEEKLY_SOFTWARE_GET$Game <- str_replace(SALES_WEEKLY_SOFTWARE_GET$Game,"\\(2015\\)","\\[2015\\]")

# More tidy 
SALES_WEEKLY_SOFTWARE_GET <- separate(SALES_WEEKLY_SOFTWARE_GET, Game, into = c("Game", "Platform.Untidy"), sep = "\\(")
SALES_WEEKLY_SOFTWARE_GET <- separate(SALES_WEEKLY_SOFTWARE_GET, Platform.Untidy, into = c("Platform", "Genre.Untidy"), sep = "\\)")
SALES_WEEKLY_SOFTWARE_GET$Genre <- str_extract(SALES_WEEKLY_SOFTWARE_GET$Genre.Untidy, "\\, [A-z]+\\-?[A-z]*")
SALES_WEEKLY_SOFTWARE_GET$Genre <- str_replace(SALES_WEEKLY_SOFTWARE_GET$Genre,"\\, ","")

# From the beginning, the way the table was scraped there is always a duplicate line for each observation. Need to filter those out. 
SALES_WEEKLY_SOFTWARE_GET[is.na(SALES_WEEKLY_SOFTWARE_GET)] <- 0
SALES_WEEKLY_SOFTWARE_GET <- filter(SALES_WEEKLY_SOFTWARE_GET, Pos != 0)
SALES_WEEKLY_SOFTWARE_GET <- SALES_WEEKLY_SOFTWARE_GET[,c("Pos", "Game", "Platform", "Genre", "Weekly", "Total", "Week.Number")]

# Convert numbers from character to integer for later use. Pro and N/A signified that membership was required to get those numbers. Put those to zero first to be later noted.
SALES_WEEKLY_SOFTWARE_GET$Weekly <- str_replace_all(SALES_WEEKLY_SOFTWARE_GET$Weekly,"\\,","")
SALES_WEEKLY_SOFTWARE_GET$Weekly <- str_replace(SALES_WEEKLY_SOFTWARE_GET$Weekly,"Pro","0")
SALES_WEEKLY_SOFTWARE_GET$Weekly <- as.integer(SALES_WEEKLY_SOFTWARE_GET$Weekly)
SALES_WEEKLY_SOFTWARE_GET$Total <- str_replace_all(SALES_WEEKLY_SOFTWARE_GET$Total,"\\,","")
SALES_WEEKLY_SOFTWARE_GET$Total <- str_replace(SALES_WEEKLY_SOFTWARE_GET$Total,"N\\/A","0")
SALES_WEEKLY_SOFTWARE_GET$Total <- as.integer(SALES_WEEKLY_SOFTWARE_GET$Total)

# The data doesn't have the actual date. Using the url, which has code that can be used to interpret the date, I can thus get the date and append this if needed. The number needs to be subtracted by three to reach the same date as on the website. Not entirely sure why this is the case since the data is not being posted three days later but weeks later. 
SALES_WEEKLY_SOFTWARE_GET$Week.Ending <- as.Date(42372 - 3, origin = "1900-01-01")
SALES_CONSOLE_GET$Week.Ending <- as.Date(42372 - 3, origin = "1900-01-01")
SALES_TOTAL_SOFTWARE_GET$Week.Ending <- as.Date(42372 - 3, origin = "1900-01-01")







```
